{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-24 18:01:58.165596: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2025-10-24 18:01:58.218051: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-10-24 18:02:00.855837: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import subprocess\n",
        "import shutil\n",
        "\n",
        "# Standard Utility Libraries\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "# TensorFlow/Keras Deep Learning Libraries\n",
        "from tensorflow.keras.utils import to_categorical, plot_model\n",
        "from tensorflow.keras.models import Sequential, save_model\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Configuration ---\n",
        "# Set the desired image size (will be cropped/resized to this)\n",
        "SIZE = 128\n",
        "# Configuration for the Kaggle Dataset\n",
        "KAGGLE_DATASET_ID = 'alxmamaev/flowers-recognition'\n",
        "DESTINATION_DIR = './Image_CLF_Datasets/'\n",
        "FLOWERS_DIR = os.path.join(DESTINATION_DIR, 'flowers/')\n",
        "# --- End Configuration ---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # create config dir\n",
        "# mkdir -p ~/.config/kaggle\n",
        "\n",
        "# # move kaggle.json (example: you uploaded it to the workspace root)\n",
        "# mv /workspaces/flower-detection/kaggle.json ~/.config/kaggle/\n",
        "\n",
        "# # set secure perms and correct ownership to current user\n",
        "# chmod 600 ~/.config/kaggle/kaggle.json\n",
        "# sudo chown $(id -un):$(id -gn) ~/.config/kaggle -R\n",
        "\n",
        "# # verify\n",
        "# ls -l ~/.config/kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Colab-aware kaggle.json setup (replace previous kaggle checks with this cell)\n",
        "import os, sys, json, shutil\n",
        "\n",
        "def install_kaggle_json(src_path):\n",
        "    dest_dir = os.path.expanduser(\"~/.kaggle\")\n",
        "    os.makedirs(dest_dir, exist_ok=True)\n",
        "    dst = os.path.join(dest_dir, \"kaggle.json\")\n",
        "    shutil.copy(src_path, dst)\n",
        "    os.chmod(dst, 0o600)\n",
        "    with open(dst, \"r\") as fh:\n",
        "        data = json.load(fh)\n",
        "    # set env vars for kaggle CLI fallback\n",
        "    if \"username\" in data and \"key\" in data:\n",
        "        os.environ[\"KAGGLE_USERNAME\"] = data[\"username\"]\n",
        "        os.environ[\"KAGGLE_KEY\"] = data[\"key\"]\n",
        "    elif \"user\" in data and \"token\" in data:\n",
        "        os.environ[\"KAGGLE_USERNAME\"] = data[\"user\"]\n",
        "        os.environ[\"KAGGLE_KEY\"] = data[\"token\"]\n",
        "    print(f\"Installed kaggle.json to {dst} and set KAGGLE_USERNAME/KAGGLE_KEY environment variables.\")\n",
        "\n",
        "# common locations to check (Colab, Codespace, workspace root)\n",
        "candidates = [\n",
        "    os.path.expanduser(\"~/.kaggle/kaggle.json\"),\n",
        "    \"/root/.kaggle/kaggle.json\",\n",
        "    \"/content/kaggle.json\",\n",
        "    \"/content/drive/MyDrive/kaggle.json\",\n",
        "    \"/workspaces/flower-detection/kaggle.json\",\n",
        "    \"./kaggle.json\"\n",
        "]\n",
        "\n",
        "found = False\n",
        "for p in candidates:\n",
        "    if p and os.path.exists(p):\n",
        "        try:\n",
        "            install_kaggle_json(p)\n",
        "            found = True\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(\"Failed to install from\", p, \":\", e)\n",
        "\n",
        "# If running in Colab and not found, prompt upload\n",
        "if not found and \"google.colab\" in sys.modules:\n",
        "    from google.colab import files\n",
        "    print(\"No kaggle.json found. Please upload kaggle.json (use the file dialog).\")\n",
        "    uploaded = files.upload()\n",
        "    if \"kaggle.json\" in uploaded:\n",
        "        with open(\"kaggle.json\", \"wb\") as f:\n",
        "            f.write(uploaded[\"kaggle.json\"])\n",
        "        install_kaggle_json(\"kaggle.json\")\n",
        "        found = True\n",
        "\n",
        "if not found:\n",
        "    # final fallback: check env vars\n",
        "    if os.environ.get(\"KAGGLE_USERNAME\") and os.environ.get(\"KAGGLE_KEY\"):\n",
        "        print(\"Kaggle credentials provided via environment variables.\")\n",
        "        found = True\n",
        "    else:\n",
        "        raise SystemExit(\n",
        "            \"Kaggle credentials not found. Place kaggle.json in ~/.kaggle/ or upload it in Colab, \"\n",
        "            \"or set KAGGLE_USERNAME and KAGGLE_KEY environment variables.\"\n",
        "        )\n",
        "\n",
        "# Now safe to import kaggle\n",
        "import kaggle\n",
        "print(\"Kaggle API ready.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Kaggle API configured via ~/.config/kaggle/kaggle.json\n"
          ]
        }
      ],
      "source": [
        "# Ensure Kaggle credentials are present before importing kaggle\n",
        "kaggle_json = os.path.expanduser(\"~/.config/kaggle/kaggle.json\")\n",
        "if os.path.exists(kaggle_json):\n",
        "    os.chmod(kaggle_json, 0o600)\n",
        "    import kaggle\n",
        "    print(\"Kaggle API configured via ~/.config/kaggle/kaggle.json\")\n",
        "elif os.environ.get(\"KAGGLE_USERNAME\") and os.environ.get(\"KAGGLE_KEY\"):\n",
        "    import kaggle\n",
        "    print(\"Kaggle API configured via environment variables\")\n",
        "else:\n",
        "    raise SystemExit(\n",
        "        \"Kaggle credentials not found. Place kaggle.json in ~/.config/kaggle/ or set KAGGLE_USERNAME/KAGGLE_KEY.\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Kaggle API successfully imported.\n"
          ]
        }
      ],
      "source": [
        "## 1. Data Fetching and Preparation 🚀\n",
        "\n",
        "# Ensure Kaggle API is available and configured\n",
        "try:\n",
        "    import kaggle\n",
        "    print(\"Kaggle API successfully imported.\")\n",
        "except ImportError:\n",
        "    print(\"Kaggle library not found. Installing now...\")\n",
        "    subprocess.run(['pip', 'install', 'kaggle'], check=True)\n",
        "    import kaggle # Re-import after installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def download_and_extract_kaggle_dataset(dataset_id, dest_dir):\n",
        "    \"\"\"Downloads and extracts the Kaggle dataset.\"\"\"\n",
        "    \n",
        "    os.makedirs(dest_dir, exist_ok=True)\n",
        "    \n",
        "    # Use the Kaggle CLI to download the dataset\n",
        "    print(f\"Downloading dataset: {dataset_id}...\")\n",
        "    try:\n",
        "        # Downloads the zip file to the destination directory\n",
        "        subprocess.run(['kaggle', 'datasets', 'download', '-d', dataset_id, '-p', dest_dir], check=True)\n",
        "        print(\"Download complete.\")\n",
        "    except subprocess.CalledProcessError:\n",
        "        print(\"\\nERROR: Kaggle download failed. Please check credentials and connection.\")\n",
        "        print(\"Ensure the 'kaggle.json' file is correctly placed in ~/.kaggle/ for API authentication.\")\n",
        "        return False\n",
        "        \n",
        "    # Find and extract the downloaded zip file\n",
        "    zip_files = [f for f in os.listdir(dest_dir) if f.endswith('.zip')]\n",
        "    if not zip_files:\n",
        "        print(\"Error: Downloaded zip file not found.\")\n",
        "        return False\n",
        "\n",
        "    zip_path = os.path.join(dest_dir, zip_files[0])\n",
        "    \n",
        "    print(f\"Extracting {zip_files[0]}...\")\n",
        "    shutil.unpack_archive(zip_path, dest_dir)\n",
        "    os.remove(zip_path) # Clean up the zip file\n",
        "    print(f\"Data ready at: {FLOWERS_DIR}\")\n",
        "    \n",
        "    # Renaming the extracted folder if necessary (specific to this dataset)\n",
        "    extracted_folder_name = 'flowers'\n",
        "    if extracted_folder_name not in os.listdir(dest_dir):\n",
        "        # A common issue is the folder name being \"flowers-recognition\" or similar\n",
        "        print(\"Attempting to locate flower images...\")\n",
        "        \n",
        "    return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset not found locally. Starting download and extraction...\n",
            "Downloading dataset: alxmamaev/flowers-recognition...\n",
            "Dataset URL: https://www.kaggle.com/datasets/alxmamaev/flowers-recognition\n",
            "License(s): unknown\n",
            "Downloading flowers-recognition.zip to ./Image_CLF_Datasets\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 225M/225M [00:00<00:00, 1.31GB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Download complete.\n",
            "Extracting flowers-recognition.zip...\n",
            "Data ready at: ./Image_CLF_Datasets/flowers/\n"
          ]
        }
      ],
      "source": [
        "# Check if data is already present before downloading\n",
        "if not os.path.isdir(FLOWERS_DIR) or not os.listdir(FLOWERS_DIR):\n",
        "    print(\"Dataset not found locally. Starting download and extraction...\")\n",
        "    success = download_and_extract_kaggle_dataset(KAGGLE_DATASET_ID, DESTINATION_DIR)\n",
        "    if not success:\n",
        "        raise SystemExit(\"Dataset setup failed. Cannot proceed with training.\")\n",
        "else:\n",
        "    print(f\"Dataset already found at {FLOWERS_DIR}. Skipping download.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Detected 5 classes: ['daisy' 'dandelion' 'rose' 'sunflower' 'tulip']\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# --- Data Loading and Preprocessing ---\n",
        "data = []\n",
        "label = []\n",
        "\n",
        "# Assuming the 'flowers' folder contains subfolders, where each subfolder name is a class label.\n",
        "for folder in os.listdir(FLOWERS_DIR):\n",
        "    folder_path = os.path.join(FLOWERS_DIR, folder)\n",
        "    if os.path.isdir(folder_path):\n",
        "        for file in os.listdir(folder_path):\n",
        "            if file.endswith(\"jpg\"):\n",
        "                label.append(folder)\n",
        "                # Read, convert (BGR to RGB), and resize image\n",
        "                img = cv2.imread(os.path.join(folder_path, file))\n",
        "                if img is not None:\n",
        "                    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "                    im = cv2.resize(img_rgb, (SIZE, SIZE))\n",
        "                    data.append(im)\n",
        "\n",
        "# Convert data into numerical values\n",
        "X = np.array(data)\n",
        "label_arr = np.array(label)\n",
        "\n",
        "# Use label encoder and one-hot encode the data\n",
        "encoder = LabelEncoder()\n",
        "y_encoded = encoder.fit_transform(label_arr)\n",
        "\n",
        "# Get the list of class names in the order they were encoded\n",
        "categories = encoder.classes_\n",
        "NUM_CLASSES = len(categories)\n",
        "print(f\"\\nDetected {NUM_CLASSES} classes: {categories}\")\n",
        "\n",
        "# One-hot encoding\n",
        "y = to_categorical(y_encoded, NUM_CLASSES)\n",
        "\n",
        "# Normalize the image data (scaling pixel values to [0, 1])\n",
        "X = X / 255.0\n",
        "\n",
        "# Split the dataset into 70% training and 30% testing set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
        "print(f\"Train samples: {X_train.shape[0]}, Test samples: {X_test.shape[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 2. Build Convolutional Neural Network (CNN) Architecture 🏗️\n",
        "\n",
        "model = Sequential(name='Flower_Classifier_CNN')\n",
        "\n",
        "# First Convolutional Block\n",
        "model.add(Conv2D(filters=64, kernel_size=(3,3), padding='Same', activation='relu', input_shape=(SIZE, SIZE, 3), name='Conv_1_64'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2), name='Pool_1'))\n",
        "\n",
        "# Second Convolutional Block (Triple Conv Layers)\n",
        "model.add(Conv2D(filters=128, kernel_size=(3,3), padding='Same', activation='relu', name='Conv_2_128_a'))\n",
        "model.add(Conv2D(filters=128, kernel_size=(3,3), padding='Same', activation='relu', name='Conv_2_128_b'))\n",
        "model.add(Conv2D(filters=128, kernel_size=(3,3), padding='Same', activation='relu', name='Conv_2_128_c'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2), name='Pool_2'))\n",
        "\n",
        "# Fully Connected Layers (Classifier)\n",
        "model.add(Flatten(name='Flatten_Layer'))\n",
        "model.add(Dense(128, activation='relu', name='Dense_128'))\n",
        "model.add(Dense(64, activation='relu', name='Dense_64'))\n",
        "model.add(Dropout(rate=0.25, name='Dropout_0_25'))\n",
        "# Output Layer\n",
        "model.add(Dense(NUM_CLASSES, activation='softmax', name='Output_5_Classes'))\n",
        "\n",
        "print(\"\\n--- Model Summary ---\")\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate model plot (requires pydot and graphviz)\n",
        "try:\n",
        "    plot_model(model, to_file='model_architecture.png', show_shapes=True, show_layer_names=True)\n",
        "    print(\"\\nModel architecture saved to model_architecture.png\")\n",
        "except ImportError:\n",
        "    print(\"\\nWarning: pydot or graphviz not installed. Skipping model plot generation.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 3. Data Augmentation and Model Training ⚙️\n",
        "\n",
        "# Image Data Augmentation settings\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    zoom_range=0.20,\n",
        "    width_shift_range=0.3,\n",
        "    height_shift_range=0.3,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "datagen.fit(X_train)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=0.0001),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "batch_size = 32\n",
        "epochs = 64\n",
        "\n",
        "# Use Model.fit() which replaces the deprecated Model.fit_generator\n",
        "print(\"\\n--- Model Training Started ---\")\n",
        "history = model.fit(\n",
        "    datagen.flow(X_train, y_train, batch_size=batch_size),\n",
        "    epochs=epochs,\n",
        "    validation_data=(X_test, y_test),\n",
        "    verbose=2\n",
        ")\n",
        "print(\"--- Model Training Finished ---\")\n",
        "\n",
        "# Save the trained model artifact\n",
        "save_model(model, 'flower_classifier_model.h5')\n",
        "print(\"\\nTrained model saved as 'flower_classifier_model.h5'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 4. Evaluation and Visualization 📊\n",
        "\n",
        "# --- A. Plot Training History ---\n",
        "def plot_history(history):\n",
        "    \"\"\"Plots the training and validation loss and accuracy history.\"\"\"\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    # Plot training & validation accuracy values\n",
        "    ax1.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "    ax1.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    ax1.set_title('Model Accuracy')\n",
        "    ax1.set_ylabel('Accuracy')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.legend(loc='lower right')\n",
        "    ax1.grid(True)\n",
        "\n",
        "    # Plot training & validation loss values\n",
        "    ax2.plot(history.history['loss'], label='Train Loss')\n",
        "    ax2.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    ax2.set_title('Model Loss')\n",
        "    ax2.set_ylabel('Loss')\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.legend(loc='upper right')\n",
        "    ax2.grid(True)\n",
        "    \n",
        "    plt.show()\n",
        "\n",
        "plot_history(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- B. Visualize Predictions on Test Set ---\n",
        "\n",
        "# Run prediction once for efficiency\n",
        "y_pred_probs = model.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred_probs, axis=1)\n",
        "y_true_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Display 36 random test images with predictions\n",
        "fig, ax = plt.subplots(6, 6, figsize=(25, 25))\n",
        "fig.suptitle(\"CNN Flower Classification Results\", fontsize=24)\n",
        "\n",
        "for i in range(6):\n",
        "    for j in range(6):\n",
        "        k = int(np.random.random_sample() * len(X_test))\n",
        "        \n",
        "        true_label = categories[y_true_classes[k]]\n",
        "        pred_label = categories[y_pred_classes[k]]\n",
        "\n",
        "        is_correct = (true_label == pred_label)\n",
        "        color = 'green' if is_correct else 'red'\n",
        "\n",
        "        ax[i, j].set_title(f\"TRUE: {true_label}\", color=color)\n",
        "        ax[i, j].set_xlabel(f\"PREDICTED: {pred_label}\", color=color)\n",
        "        ax[i, j].imshow(X_test[k]) \n",
        "        ax[i, j].axis('off')\n",
        "\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "plt.show()\n",
        "\n",
        "# --- C. Plot Confusion Matrix ---\n",
        "\n",
        "cm = confusion_matrix(y_true_classes, y_pred_classes)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=categories, yticklabels=categories)\n",
        "plt.title('Confusion Matrix')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
